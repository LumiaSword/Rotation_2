---
title: "R Notebook"
output: html_notebook
---


# Notes

The second rotation is centered around the downstream data analysis of ribo-seq. Ribosomal analysis was performed by sequencing the Ribosome Protected Footprints (RPFs), which reflects the status of the ribosome through the mRNA fragments undergoing translation, thus obtaining a distribution of reads of a specific length at the transcript level. Data were analyzed using the RiboR software package.

## Updates
1. Review ribosome-profiling. Learn how to use RiboR. Go through tutorial.

2. Use real .ribo data. Filter for the top 100 transcripts with the highest read counts. Check their coverage in each CDS regions. Visualize the distribution.

3. Adjust the way bins are divided.

    before: Position/transcript cds region length to get a relative length. Relative length * 100 and downward rounding to get position assigned to each bin. 
    
    after: A determined number of bins are set. Transcripts with different region lengths have different position ranges in the each bin.
4. For each bins, the distribution of all transcripts is represented by adding up ~~raw counts~~ their respective densities;

    The distribution map is extended to the full transcript range.

    Decide how to set different numbers of bins for different regions.

    Rearrange the order of heatmaprows. Cluster between transcripts.

    Observe outliers transcripts. Try to explain.

5. Backed up using git; add comments; for WDR74 and MTRNR2L, documented and excluded from subsequent analyses

    The correctness of the method was verified using a test set.

    Try to demonstrate the read distribution for higher precision regions.

    If the method is correct, try to explain the high enrichment of readings in the UTR region.

    After confirming the flow of the method, apply it to all 6 experiments.

    Observe if there is a consistent trend in the expression of transcripts across the six experiments?

    How can we programmatically identify differences in trends?
    
    For NPMI-201 and PPIA-204, why they are partially not expressed in the CDS region?

6. Do simple test case. Check again whether the reads are assigned to correct bins.

    Change the calculation of density based on from the total count in the certain region to the whole across the transcipt.
    
    See if there is a consistent trend in the distribution of single transcripts across the six experiments? What are the subjective judgments? How can this process be quantified?
    
    Change correlation method from Pearson to Spearman. Explain why.
    
    Check the uniformity of each transcripts. Why?
    
    Find the interesting genes. Explain why the distribution looks like that.

# Packages

```{r}
library(ribor)
library(tidyverse)
library(ggplot2)
library(viridis)
library(pheatmap)
library(grid)
library(ggplotify)
library(cowplot)
library(broom)
library(purrr)
library(svglite)
```

# Data sources

Open source data from [NCBI](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158374). .ribo file are generated using [riboflow](https://github.com/ribosomeprofiling/riboflow).

```{r}
h1299.ribo = Ribo("GSE158374_H1299.ribo.hdf5", rename = rename_default )
hek293.ribo = Ribo("GSE158374_manuscript.HEK293.ribo.hdf5",rename = rename_default )
```

# H1299

## Basic info of the .ribo file
```{r}
h1299.ribo

# coverage TRUE, rna TRUE, metadata FALSE
```


```{r,eval = FALSE}
h1299.start = get_metagene(ribo.object = h1299.ribo, 
                           site        = "start",
                           range.lower = 28,
                           range.upper = 32,
                           length      = FALSE,
                           transcript  = FALSE,
                           alias       = TRUE) # Coverage check.

h1299.start[1:5,c(1,2,3,53,54,55)]

head(get_reference_names(h1299.ribo), 2)
```


```{r}
# Save as SVG
# svglite("region_count.svg", width = 11, height = 6)
plot_region_counts(x           = h1299.ribo,
                   range.lower = 28,
                   range.upper = 32) # Read Counts for different experiments

# dev.off()
```



```{r}
h1299.rc = get_region_counts(h1299.ribo,
                        range.lower = 28, # usually the RPFs has a 28-30nt in length. Use 28-32 for here
                        range.upper = 32,
                        length      = T, # don't separate based on different length
                        transcript  = F, # transcriptome level result
                        alias = T,  # Name of the transcript
                        compact = F) # data.fraame rather than DataFrame
```


```{r, eval=FALSE}
top100_h1299_cds = h1299.rc %>%
  filter(region == "CDS") %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count))%>%
  head(100) %>%
  pull(transcript) # This will return the top 100 transcripts with the most reads in CDS region
```


```{r}
top100_h1299 = h1299.rc %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  head(100) %>%
  pull(transcript) # This will return the top 100 transcripts wih the most reads amoung all region, including UTR5, CDS and UTR3, across all 6 experiments.
```

```{r}
# Check the distribution of those 28-32nt reads in the top 100 broadly
rc_top100 = get_region_counts(h1299.ribo, # return the reads count for UTR, UTRJ, and CDS
                        range.lower = 28,
                        range.upper = 32,
                        length      = T,
                        transcript  = F,
                        alias = T,
                        compact = F,
                        tidy = F) %>%
  filter(transcript %in% top100_h1299)

rc_top100 # for all 6 experiments
```


```{r}
region_coord = get_original_region_coordinates(ribo.object = h1299.ribo , alias = TRUE) # Find the region coordinates for h1299.

top100_region_coord = region_coord %>% # filter for the top 100 transcripts
    filter(transcript %in% top100_h1299) 

# top100_cds_region_coord = region_coord %>%
#     filter(transcript %in% top100_h1299_cds) 

top100_region_coord %>%
  filter(str_detect(transcript,"ACT"))
```

## Check if the variable indicate the right bonds
```{r,eval=FALSE}
top100_region_coord %>% # manually calculate the original region bonds
  group_by(transcript) %>%
  summarise(UTR5 = UTR5_stop - UTR5_start+1,CDS = CDS_stop - CDS_start+1, UTR3 = UTR3_stop - UTR3_start+1) %>%
  ungroup()
```


```{r,eval=FALSE}
get_original_region_lengths(ribo.object = h1299.ribo, # check with integreted algorithm
                            alias = T) %>%
  separate(transcript, into = paste0("part", 1:5), sep = "\\|", fill = "right") %>%
  filter(part5 %in% top100_h1299) %>%
  select(part5,UTR5,CDS,UTR3) %>%
  arrange(part5)
```

It looks like the bonds is correct.


# A function for top100 distribution in all 6 experiments

```{r}
experiment.info = get_info(ribo.object = h1299.ribo)[['experiment.info']]
has.coverage = experiment.info[experiment.info$coverage == TRUE, "experiment"]
```


To write a function, the intake will be:

    0. An empty list

    1. A list of transcripts name (top 100).

    2. A coordinate df showing transcripts name, CDS start, CDS end, and UTR3 end.

    3. Bin size for each region.

    4. Set of experiments.

The info above, together with the ribo file, can generate coverage df: 

    Coverage df: experiment name, position (from 1 to UTR3 end), count (for reads at each position).



```{r}
# Function to calculate distribution
cov_cal = function(ribo_name,
                   transcripts_names,
                   transcripts_coordinates_df,
                   exp.names,
                   n_bins_utr5,
                   n_bins_cds,
                   n_bins_utr3){
  
  cov_list_exp = list()
  
  for (i in 1:length(exp.names)){
    cov_list = list()
    
    for (j in 1:length(transcripts_names)){
      # cds region bond
      cds_start = transcripts_coordinates_df %>%
        filter(transcript == transcripts_names[j]) %>%
        pull(CDS_start)
      
      cds_end = transcripts_coordinates_df %>%
        filter(transcript == transcripts_names[j]) %>%
        pull(CDS_stop)
      
      # utr5 region bond
      utr5_start = 1
      utr5_end = cds_start - 1
  
      # utr3 region bond
      utr3_start = cds_end + 1
      utr3_end = transcripts_coordinates_df %>%
        filter(transcript == transcripts_names[j]) %>%
        pull(UTR3_stop)
  
      # region length
      utr5_length = utr5_end - utr5_start + 1
      cds_length = cds_end - cds_start + 1
      utr3_length = utr3_end - utr3_start + 1
      
      # bin_size for each transcript
      bin_size_utr5 = utr5_length / n_bins_utr5
      bin_size_cds = cds_length / n_bins_cds
      bin_size_utr3 = utr3_length / n_bins_utr3
      
      # generate the coverage df
      cov_df = get_coverage(ribo.object = ribo_name,
                            name        = transcripts_names[j],
                            range.lower = 28,
                            range.upper = 32,
                            length      = T,
                            alias       = TRUE,
                            tidy        = TRUE,
                            compact     = F,
                            experiment  = exp.names[i])
      
      # reconstruct the coverage df
      cov_df = cov_df %>%
        mutate(position = as.numeric(position)) %>% # as.numeric the position
        mutate(region = case_when(
          position >= utr5_start & position <= utr5_end ~ "UTR5", 
          position >= cds_start & position <= cds_end ~ "CDS",
          position >= utr3_start & position <= utr3_end ~ "UTR3", # assign region for each position
          TRUE ~ NA_character_
          )) %>%
        filter(!is.na(region)) %>%
        mutate(relative_position = case_when(
          region == "UTR5" ~ (position - utr5_start + 1),
          region == "CDS" ~ (position - cds_start + 1),
          region == "UTR3" ~ (position - utr3_start + 1) # calculate the relative position for each region
          )) %>%
        mutate(bin = case_when(
          region == "UTR5" ~ ceiling(relative_position / bin_size_utr5),
          region == "CDS" ~ ceiling(relative_position / bin_size_cds),
          region == "UTR3" ~ ceiling(relative_position / bin_size_utr3) # assign the bin for each position in each region
          ))
      
      # adjust the bin if there are position assigned to bins out of the range
      cov_df$bin[cov_df$region == "UTR5" & cov_df$bin > n_bins_utr5] = n_bins_utr5
      cov_df$bin[cov_df$region == "CDS" & cov_df$bin > n_bins_cds] = n_bins_cds
      cov_df$bin[cov_df$region == "UTR3" & cov_df$bin > n_bins_utr3] = n_bins_utr3
      
      # now the cov_df should have there col: experiment, position, reads, region, relative_position, bin
      # only keep the region and bin, and add transcript for the df
      all_cov = cov_df %>%
        group_by(region, bin) %>%
        summarise(total_count = sum(count),.groups = "drop") %>%
        mutate(transcript = transcripts_names[j]) 
      
      
      # fill the df into the list
      cov_list[[transcripts_names[j]]] = all_cov
    }
    
    # get the df for a certain experiment of all selected transcripts
    coverage_all = bind_rows(cov_list)
    
    coverage_all = coverage_all %>%
      mutate(region = factor(region, levels = c("UTR5", "CDS", "UTR3"))) # the following analysis can thus have the order 
    
    # save the coverage data for the current experiment
    cov_list_exp[[exp.names[i]]] = coverage_all
  }
  
  # return the final list containing coverage data for all experiments
  return(cov_list_exp)
}
```


```{r}
h1299_cov = cov_cal(ribo_name = h1299.ribo,
            transcripts_names = top100_h1299,
            transcripts_coordinates_df = top100_region_coord,
            exp.names = has.coverage,
            n_bins_utr5 = 30,
            n_bins_cds = 100,
            n_bins_utr3 = 30)
```

## To manually check whether the function works

```{r,eval=FALSE}
cov_list = list()

n_bins_utr5 = 30
n_bins_cds = 100
n_bins_utr3 = 30 # different bins number set for different region

# experiment: 20210318-NSP1-H1299-A
for (i in 1:length(top100_h1299)) {
  # cds region bond
  cds_start = top100_region_coord %>%
    filter(transcript == top100_h1299[i]) %>%
    pull(CDS_start)
  
  cds_end = top100_region_coord %>%
    filter(transcript == top100_h1299[i]) %>%
    pull(CDS_stop)
  
  # utr5 region bond
  utr5_start = 1
  utr5_end = cds_start - 1
  
  # utr3 region bond
  utr3_start = cds_end + 1
  utr3_end = top100_region_coord %>%
    filter(transcript == top100_h1299[i]) %>%
    pull(UTR3_stop)
  
  utr5_length = utr5_end - utr5_start + 1
  cds_length = cds_end - cds_start + 1
  utr3_length = utr3_end - utr3_start + 1
  
  bin_size_utr5 = utr5_length / n_bins_utr5
  bin_size_cds = cds_length / n_bins_cds
  bin_size_utr3 = utr3_length / n_bins_utr3
  
  cov_df = get_coverage(ribo.object = h1299.ribo,
                    name        = top100_h1299[i],
                    range.lower = 28,
                    range.upper = 32,
                    length      = T,
                    alias       = TRUE,
                    tidy        = TRUE,
                    compact     = F,
                    experiment  = has.coverage[6])
  
  cov_df = cov_df %>%
    mutate(position = as.numeric(position)) %>%
    mutate(region = case_when(
      position >= utr5_start & position <= utr5_end ~ "UTR5",
      position >= cds_start & position <= cds_end ~ "CDS",
      position >= utr3_start & position <= utr3_end ~ "UTR3",
      TRUE ~ NA_character_
    )) %>%
    filter(!is.na(region)) %>%
    mutate(relative_position = case_when(
      region == "UTR5" ~ (position - utr5_start + 1),
      region == "CDS" ~ (position - cds_start + 1),
      region == "UTR3" ~ (position - utr3_start + 1)
    )) %>%
    mutate(bin = case_when(
      region == "UTR5" ~ ceiling(relative_position / bin_size_utr5),
      region == "CDS" ~ ceiling(relative_position / bin_size_cds),
      region == "UTR3" ~ ceiling(relative_position / bin_size_utr3)
    ))
  
  cov_df$bin[cov_df$region == "UTR5" & cov_df$bin > n_bins_utr5] = n_bins_utr5
  cov_df$bin[cov_df$region == "CDS" & cov_df$bin > n_bins_cds] = n_bins_cds
  cov_df$bin[cov_df$region == "UTR3" & cov_df$bin > n_bins_utr3] = n_bins_utr3
  
  all_cov = cov_df %>%
    group_by(region, bin) %>%
    summarise(total_count = sum(count),.groups = "drop") %>%
    mutate(transcript = top100_h1299[i]) 
    
  
  cov_list[[top100_h1299[i]]] = all_cov
  
}


coverage_all = bind_rows(cov_list)

coverage_all = coverage_all %>%
  mutate(region = factor(region, levels = c("UTR5", "CDS", "UTR3")))
```


```{r,eval=FALSE}
all(coverage_all == h1299_cov[[6]])
```

# Visualize

```{r}
# Define the color-blind-friendly palette
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


## Distribution, by density
```{r}
# The first experiment, as an example, 20210318-NSP1-H1299-A
# Save as SVG
# svglite("distribution_1.svg", width = 11, height = 6)

h1299_cov_exp1 = h1299_cov[[1]]

h1299_cov_exp1 %>%
  group_by(transcript) %>%
  mutate(density = total_count / sum(total_count))  %>% # density will turn NA if total_count in certain region to be 0
  ungroup() %>%
  group_by(region,bin) %>%
  summarise(count = sum(density,na.rm = T),.groups = "drop") %>%
  ggplot(aes(x = bin, y = count)) +
  geom_point(color = "#0072B2") + # color-blind friendly
  geom_line(color = "#0072B2") +
  labs(
    title = "Overall Read Count Density Across Regions",
    x = "Bin",
    y = "Sum Read Count Density"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    axis.text = element_text(size = 12),                       # Axis labels
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines
  ) +
  facet_grid(~ region, scales = "free_x", space = "free")

# Close the SVG device
# dev.off()

# coverage_all %>%
#   group_by(region,bin) %>%
#   summarise(count = sum(total_count),.groups = "drop") %>%
#   ggplot(aes(x = bin, y = count)) +
#   geom_line() +
#   labs(
#     x = "Bin",
#     y = "Sum Read Count"
#   ) +
#   facet_wrap(~ region, scales = "free_x")
```

```{r}
# svglite("metagene_start.svg", width = 12, height = 6)
plot_metagene(h1299.ribo,
              site        = "start",
              experiment  = has.coverage,
              range.lower = 28,
              range.upper = 32)
# dev.off()
```

```{r}
# svglite("metagene_stop.svg", width = 12, height = 6)
plot_metagene(h1299.ribo,
              site        = "stop",
              experiment  = has.coverage,
              range.lower = 28,
              range.upper = 32)
# dev.off()
```



> We can see the CDS has a significant higher distribution in reads, which is align with our expectation.
>
> But also, we can find something interesting around the start point. We should expect accumulation here, but should be more accumulated in the right hand of start point -- that is, in the CDS region. However, we can see now there's a peak near the start point in UTR5 region, which is way higher than the one in CDS.
>


```{r}
# combined version for all six experiments
combined_cov = bind_rows(
  lapply(1:length(h1299_cov), function(i) {
    h1299_cov[[i]] %>%
      mutate(experiment = has.coverage[i])
  })
)
```


```{r}
# svglite("distribution_all.svg", width = 12, height = 6)

combined_cov %>%
  mutate(experiment = str_replace(experiment, "20210318-(NSP[12])-H1299-([A-C])", "\\1-\\2")) %>%
  group_by(transcript, experiment) %>%
  mutate(density = total_count / sum(total_count)) %>%
  ungroup() %>%
  group_by(region, bin, experiment) %>%
  summarise(count = sum(density, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = bin, y = count, colour = experiment)) +
  geom_line(linewidth = 0.5) +
  labs(
    title = "Read Count Density Across Regions and experiments in H1299 Cells",
    x = "Bin",
    y = "Sum Read Count Density"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    axis.text = element_text(size = 12),                       # Axis labels
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines

  ) +
  facet_grid(~ region, scales = "free_x", space = "free") # each row an experiment

# dev.off()
```




## Heatmap Distribution

> Would it be a good idea to use density to represent the reads number in each bin of each transcript in a heatmap? No.

```{r}
# svglite("heatmap_1.svg", width = 16, height = 16)

h1299_cov_exp1 %>%
  mutate(transcript = factor(transcript, levels = rev(top100_h1299)))%>%
  ggplot(aes(x = bin, y = transcript, fill = log10(total_count+1)))+
  geom_tile() +
  labs(
    x = "Bin",
    y = "Transcript",
    fill = "Read Count")+
  theme_test() + 
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5), # Main title
    axis.title.x = element_text(size = 14, face = "bold"), # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"), # Y-axis title
    axis.text = element_text(size = 12), # Axis labels
    strip.text = element_text(size = 14, face = "bold"), # Facet label
    legend.text = element_text(size = 12), # legend text
    legend.title = element_text(size = 14) # legend title
  ) +
  guides(
    fill = guide_colourbar(barheight = 10, barwidth = 1)  # 拉长彩条，调整高度和宽度
  ) +
  scale_fill_viridis_c()  + 
  facet_grid(~ region, scales = "free_x", space = "free")

# dev.off()
```

```{r}
# svglite("heatmap_1_4.svg", width = 12, height = 3)

h1299_cov_exp1 %>%
  mutate(transcript = factor(transcript, levels = rev(top100_h1299)))%>%
  filter(str_detect(transcript,"MTRNR|WDR")) %>%
  ggplot(aes(x = bin, y = transcript, fill = log10(total_count+1)))+
  geom_tile() +
  labs(
    x = "Bin",
    y = "Transcript",
    fill = "Read Count")+
  theme_test() +
  
  scale_fill_viridis_c()  + 
  facet_grid(~ region, scales = "free_x", space = "free")

# dev.off()
```


```{r}
# Heatmap clustering on trancripts
# write a function

heatmap_cluster = function(exp_data, exp_name, sorted_transcripts_names) {
  exp_data_wide = exp_data %>%
    group_by(transcript) %>%
    arrange(region) %>%
    mutate(transcript = factor(transcript, levels = sorted_transcripts_names)) %>%
    arrange(transcript) %>%
    pivot_wider(names_from = c(region, bin), values_from = total_count) # into wider form of df
  
  exp_data_matrix = as.matrix(exp_data_wide[,-1])
  rownames(exp_data_matrix) <- exp_data_wide$transcript # rm transcript col
  
  annotation_col = data.frame(
    Region = ifelse(grepl("UTR5", colnames(exp_data_matrix)), "UTR5",
                    ifelse(grepl("CDS", colnames(exp_data_matrix)), "CDS", "UTR3")) # annotate the region for the final pheatmap
  )
  rownames(annotation_col) <- colnames(exp_data_matrix)
  
  # generate heatmap
  heatmap_plot = pheatmap(
    log10(exp_data_matrix + 1),
    cluster_rows = TRUE,
    cluster_cols = F, # cluster based on rows
    show_rownames = TRUE,
    show_colnames = F, # show the transcripts' name
    color = viridis::viridis(50),
    annotation_col = annotation_col,
    main = paste(exp_name, ": Transcripts Read Count Heatmap"),
    silent = TRUE # return no plot 
  )
  
  heatmap_plot = as.ggplot(heatmap_plot)
  
  return(heatmap_plot)
}
```


```{r}
h1299_exp1_heatmap = heatmap_cluster(h1299_cov_exp1,has.coverage[1],top100_h1299)
```


```{r}
# svglite("heatmap_1.1.svg", width = 12, height = 16)
h1299_exp1_heatmap
# dev.off()
```


```{r}
heatmap_list = lapply(1:6, function(i) heatmap_cluster(h1299_cov[[i]], has.coverage[i],top100_h1299))
```


```{r,fig.height=20}
# svglite("heatmap_1.6.svg", width = 30, height = 25)
plot_grid(plotlist = heatmap_list, ncol = 3, align = 'v')
# dev.off()
```


## Troubleshooting typical anomalies

We can easily observe that MTRNR2L transcripts and WDR74 lack of reading in all 6 experiments. The reason is that MTRNR2L are just psuedo genes here.


We should exclude these transcripts and go through the whole process again.

```{r}
# Re-select the transcripts

excluded_transcripts = h1299.rc %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  head(104) %>%
  filter(str_detect(transcript,c("MTRNR")) | str_detect(transcript,"WDR")) %>%
  pull(transcript)

top100_h1299_mod = h1299.rc %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  head(104) %>%
  pull(transcript) # pull the top 104

top100_h1299_mod = setdiff(top100_h1299_mod,excluded_transcripts) # filter out the excluded outliers

top100_region_coord_mod = region_coord %>%
  filter(transcript %in% top100_h1299_mod)
```


```{r}
h1299_cov_mod = cov_cal(ribo_name = h1299.ribo,
            transcripts_names = top100_h1299_mod,
            transcripts_coordinates_df = top100_region_coord_mod,
            exp.names = has.coverage,
            n_bins_utr5 = 30,
            n_bins_cds = 100,
            n_bins_utr3 = 30)
```

```{r}
# Regenerate the Heatmap with the new objects
mod_heatmap_list = lapply(1:6, function(i) heatmap_cluster(h1299_cov_mod[[i]], has.coverage[i],top100_h1299_mod))
```


```{r,fig.height=30}
# svglite("heatmap_2.6.svg", width = 30, height = 25)
plot_grid(plotlist = mod_heatmap_list, ncol = 3, align = 'v')
# dev.off()
```


## Interesting transcripts (Are the transcripts distributed similarly across all the experiments.)

How we define whether the reads distribution of the transcripts remain similar or varies? We can first combine the df

```{r}
combined_cov_h1299_mod = bind_rows(
  lapply(1:length(h1299_cov_mod), function(i) {
    h1299_cov_mod[[i]] %>%
      mutate(experiment = has.coverage[i])
  })
)
```


We can observe that the transcripts with the highest number of readings in the CDS area are, for example

EEF1A1-202,HSP90AA1-201,GAPDH-201,VIM-209,HSP90AB1-202,HSPA8-224

Among them, EEF1A1-202 have the most reads, and always have a separate branch across all 6 transcripts (6/6), and remain the brightest color; HSP90AA1-201 are always clustered together with VIM-209 (3/6) or GAPDH-201 (2/6) -- at least they are always arranged close to each other (5/6); HSP90AB1-202 and HSPA8-224 are always clustered together (6/6).

We then check the ones that has significant deletion in CDS: *NPM1-201* and *PPIA-204*; and the one have part of deletion in CDS: ACTB-201, ACTG1-210, PABPC1-201

In all exps, NPM1-201 and PPIA-204 are clustered together, and their deletion in reads distribution remain consistent; ACTB-201 and ACTG1-210 are usually (4/6) clustered together, as they both have no reads around bin 75. PABPC1-201 have the "deletion" in a some how different area in CDS region, but still consistent across 6 exps.

There are some interesting ones: 

HSPA5-201 and ACTG1-201 have some accumulation of reads in the 5' end; 

ACTB-201 and ACTG1-201 have this accumulation around start -- which is true, but their accumulation seems higher in UTR5 region, rather than CDS region, by raw counts

*HMGB1-203* have an accumulation in the 3' end, in the very last bin.

Here's the heatmap that can show the designated distribution of a single transcript across 6 experiments

```{r}
# svglite("interesting_transcripts.svg", width = 12, height = 10)
combined_cov_h1299_mod %>%
  mutate(transcript = factor(transcript, levels = rev(top100_h1299)))  %>%
  mutate(experiment = str_replace(experiment, "20210318-(NSP[12])-H1299-([A-C])", "\\1-\\2")) %>%
  filter(str_detect(transcript,"NPM1|PPIA|HSPA5|ACTB|ACTG1|HMGB1")) %>%
  ggplot(aes(x = bin, y = experiment, fill = log10(total_count+1)))+
  geom_tile() +
  labs(
    x = "Bin",
    y = "Experiment",
    fill = "Read Count") +
  scale_fill_viridis_c() + 
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5), # Main title
    axis.title.x = element_text(size = 14, face = "bold"), # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"), # Y-axis title
    axis.text = element_text(size = 12), # Axis labels
    strip.text = element_text(size = 14, face = "bold"), # Facet label
    legend.text = element_text(size = 12), # legend text
    legend.title = element_text(size = 14), # legend title
    strip.text.y = element_text(size = 14, face = "bold", angle = 0, hjust = 0), # Rotate transcript labels

  ) +
  facet_grid(transcript ~ region, scales = "free_x", space = "free") # so result can be separated by transcripts, by rows.
# dev.off()
```

This is intuitive, but also subjective. How we can do this in an objective way


```{r}
# svglite("correlation1.svg", width = 8, height = 5)

combined_cov_h1299_mod %>%
  spread(key = experiment, value = total_count) %>%
  group_by(transcript, region) %>%
  summarise(
    correlation_value = {
      # manually choose the numeric cols
      experiment_data = select(cur_data(), starts_with("2021"))
      
      # correlate
      correlation_matrix = cor(as.matrix(experiment_data), use = "pairwise.complete.obs", method = "spearman") # We use Spearman correlation here. 
      
      # `use` is how we treat the NAs. Here will compute all possible non-NA pairs.
      
      # As we want to focus on whether the trend of the transcript varies across all experiments, and would not like to be affected by the difference in scale, we use Spearman.
      
      # Take the upper part of the correlation matrix and calculate the mean.
      mean(correlation_matrix[upper.tri(correlation_matrix)], na.rm = TRUE) # Generate one correlation value for each region. 3 for a transcript.
    }
  ) %>%
  ungroup() %>%
  arrange(correlation_value) %>% # We see a lot of UTR region having high variability. We want to focus on the CDS region.
  filter(region == "CDS")%>% 
  ggplot(aes(x = correlation_value)) +
  geom_histogram(color = "white", bins = 15) +
  geom_vline(xintercept = 0.8, color = "tomato1", size = 1.2) +
  labs(
    title = "Distribution of Correlation for top transcripts",
    x = "Correlation Value",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    text = element_text(size = 14),
    axis.text = element_text(color = "black"),
     plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines
  )
# dev.off()
```

As for the CDS region, even the most variable one got > 0.8 in correlation_value, which means they are quite consistent across all 6 experiments.


We Also want to check the uniformity for each transcript.

Coefficient of variance, Shannon entropy and uniformity index (not used since it require the reads amount to be normalized before calculation) can be used to quantify the uniformity.

```{r}
uniformity = combined_cov_h1299_mod %>%
  filter(region == "CDS") %>%
  group_by(transcript,experiment) %>%
  mutate(proportion = total_count/sum(total_count)) %>% # for calculating the Shannon Entropy
  # summarise(mean = mean(total_count),var = var(total_count),.groups = "drop") %>%
  # ggplot(aes(x = mean, y = var, color = transcript)) +
  # geom_point() +
  # theme(legend.position = "none") # for checking relationship between var and mean
  summarise(Shannon = -sum(proportion * log2(proportion), na.rm = TRUE),CoV = sd(total_count)/mean(total_count),.groups = "drop") %>%
  ungroup() 
```

We want to check the most uneven transcripts. How to do it in a relatively objective way?

By setting an objective cutoff, we need to check the overall distribution of both uniformity indices.

```{r}
cov_threshold <- quantile(uniformity$CoV, 0.95)   # CoV top 5%
shannon_threshold <- quantile(uniformity$Shannon, 0.05)  # Shannon bottom 5%
```


```{r}
# CoV Density Plot
# svglite("cov_density.svg", width = 8, height = 5)

ggplot(uniformity, aes(x = CoV)) +
  geom_density(fill = "skyblue", alpha = 0.5) +
  geom_vline(xintercept = cov_threshold, color = "red", linetype = "dashed", size = 0.6) + # Add cutoff line
  labs(
    title = "Distribution of Coefficient of Variation (CoV) for top transcripts",
    x = "CoV",
    y = "Density"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    axis.text = element_text(size = 12),                       # Axis labels
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines
  ) 
# dev.off()
```

```{r}
# Shannon Density Plot
# svglite("shannon_density.svg", width = 8, height = 5)

ggplot(uniformity, aes(x = Shannon)) +
  geom_density(fill = "lightgreen", alpha = 0.5) +
  geom_vline(xintercept = shannon_threshold, color = "red", linetype = "dashed", size = 0.6) + # Add cutoff line
  labs(
    title = "Distribution of Shannon Entropy for top transcripts",
    x = "Shannon",
    y = "Density"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    axis.text = element_text(size = 12),                       # Axis labels
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines
  ) 

# dev.off()
```

We can see tails in both distribution. In order to figure out the most "variable" transcripts, we can use a 5% threshold here.

```{r}
highlight_points <- uniformity %>%
  mutate(experiment = str_replace(experiment, "20210318-(NSP[12])-H1299-([A-C])", "\\1-\\2")) %>%
  filter(CoV > cov_threshold | Shannon < shannon_threshold)
```


```{r}
# svglite("uniformity.svg", width = 10, height = 7)

uniformity %>%
  mutate(experiment = str_replace(experiment, "20210318-(NSP[12])-H1299-([A-C])", "\\1-\\2")) %>%
  ggplot( aes(x = CoV, y = Shannon)) +
  geom_point(alpha = 0.5, color = "grey") +  # grey for all
  geom_point(data = highlight_points, aes(color = transcript, shape = experiment), size = 3) +  # color for transcripts, shape for exps

  labs(
    title = "Scatter Plot of CoV vs Shannon with Highlighted Transcripts",
    x = "Coefficient of Variation (CoV)",
    y = "Shannon Entropy",
    color = "Transcript",  
    shape = "Experiment"  
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    axis.text = element_text(size = 12),                       # Axis labels
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines
  ) 

# dev.off()
```


Larger the CoV, greater the degree of dispersion; smaller the Shannon Entropy, the less uniform it is distributed

For CoV: RPL38,PPIA-204,HNRNPU-205(reader enzyme?)

For Shannon-entropy: RPL38,PPIA-204,NPM1-201,RPLXXX,HRNPU. We mentioned earlier that PPIA and NPM1 are CDS regions with a large number of "deletion" regions

We can see that both indices share some less uniformly distributed transcripts.

One thing really interest me here is that, NPM1 seems to be more "uniform" than RPL38, although by only looking at the heatmap,intuitively speaking, I would not get such a result.

```{r,fig.width=15,fig.height= 5}
# svglite("uniformity_heatmap.svg", width = 12, height = 4)

combined_cov_h1299_mod %>%
  mutate(transcript = factor(transcript, levels = rev(top100_h1299))) %>%
  mutate(experiment = str_replace(experiment, "20210318-(NSP[12])-H1299-([A-C])", "\\1-\\2")) %>%
  filter(str_detect(transcript,"NPM1|RPL38|PPIA")) %>%
  filter(region == "CDS") %>%
  ggplot(aes(x = bin, y = experiment, fill = log10(total_count+1)))+
  geom_tile() +
  labs(
    x = "Bin",
    y = "Experiment",
    fill = "Read Count")+
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5), # Main title
    axis.title.x = element_text(size = 14, face = "bold"), # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"), # Y-axis title
    axis.text = element_text(size = 12), # Axis labels
    strip.text = element_text(size = 14, face = "bold"), # Facet label
    legend.text = element_text(size = 12), # legend text
    legend.title = element_text(size = 14), # legend title
    strip.text.y = element_text(size = 14, face = "bold", angle = 0, hjust = 0), # Rotate transcript labels

  ) +
  scale_fill_viridis_c()  + 
  facet_grid(transcript ~ region, scales = "free_x", space = "free") # so result can be separated by transcripts, by rows.

# dev.off()
```

```{r}
combined_cov_h1299_mod %>%
  filter(str_detect(transcript,"NPM1|RPL38")) %>%
  filter(region == "CDS") %>%
  group_by(transcript,experiment) %>%
  summarise(var = var(total_count),MEAN = mean(total_count),.groups = "drop") %>%
  ungroup()

uniformity %>%
  filter(str_detect(transcript,"NPM1|RPL38"))
```

Maybe bin assignment reduce the variance that raw counts brought?



# Modified Distribution

```{r}
h1299_cov_mod[[1]] %>%
  group_by(transcript) %>%
  mutate(density = total_count / sum(total_count))  %>% # density will turn NA if total_count in certain region to be 0
  ungroup() %>%
  group_by(region,bin) %>%
  summarise(count = sum(density,na.rm = T),.groups = "drop") %>%
  ggplot(aes(x = bin, y = count)) +
  geom_point() + 
  geom_line() +
  labs(
    x = "Bin",
    y = "Sum Read Count Density"
  ) +
  facet_grid(~ region, scales = "free_x", space = "free")
```


```{r}
# svglite("distribution_all_mod.svg", width = 12, height = 6)

combined_cov_h1299_mod %>%
  mutate(experiment = str_replace(experiment, "20210318-(NSP[12])-H1299-([A-C])", "\\1-\\2")) %>%
  group_by(transcript, experiment) %>%
  mutate(density = total_count / sum(total_count)) %>%
  ungroup() %>%
  group_by(region, bin, experiment) %>%
  summarise(count = sum(density, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = bin, y = count, colour = experiment)) +
  geom_line(linewidth = 0.5) +
  labs(
    title = "Read Count Density Across Regions and experiments in H1299 Cells",
    x = "Bin",
    y = "Sum Read Count Density"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 18, face = "bold",hjust = 0.5),       # Main title
    axis.title.x = element_text(size = 14, face = "bold"),     # X-axis title
    axis.title.y = element_text(size = 14, face = "bold"),     # Y-axis title
    axis.text = element_text(size = 12),                       # Axis labels
    strip.text = element_text(size = 14, face = "bold"),        # Facet label
    panel.grid.major = element_line(color = "grey90", size = 0.25),   # Major grid lines

  ) +
  facet_grid(~ region, scales = "free_x", space = "free") # each row an experiment

# dev.off()
```

We see that the abnormal peak in UTR3 caused by pseudogenes have been removed.


# Quick TE check

What is translational efficiency?

    TE is the


    
```{r}
rna_h1299 = get_rnaseq(ribo.object = h1299.ribo,
                     tidy        = TRUE,
                     compact = FALSE,
                     alias       = TRUE,
                     experiment  = has.coverage) # By checking the info, I know all 6 have coverage info as well as rna info

h1299.rc.all = get_region_counts(h1299.ribo,
                                 tidy = TRUE,
                        length      = T, # don't separate based on different length
                        transcript  = F, # transcriptome level result
                        alias = T,  # Name of the transcript
                        compact = F)
```


```{r}
a = rna_h1299 %>%
  filter(transcript %in% top100_h1299_mod) %>%
  filter(region == "CDS") %>%
  filter(experiment == "20210318-NSP1-H1299-A") %>%
  pull(count)

b = h1299.rc %>%
  filter(transcript %in% top100_h1299_mod) %>%
  filter(region == "CDS") %>%
  filter(experiment == "20210318-NSP1-H1299-A") %>%
  pull(count)

plot(log2(a), log2(b), xlab = "RNA-Seq", ylab = "Ribosome Profiling", main = "log2 CDS Read Counts", pch = 19, cex = 0.5)
```

```{r}
te = b/a
```

# Check the exact coverage

```{r}
h1299_cov_mod[[1]] %>%
  filter(str_detect(transcript,"PPIA|NPM1")) # 

get_original_region_coordinates(h1299.ribo,
                                alias = T) %>%
  filter(str_detect(transcript,"PPIA-204|NPM1")) # CDS region length: NPM1 = 882, start 302; PPIA = 494, start 46
```

Both CDS region length = 1124 (1128 in fasta), ACTG1 start 428, ACTB start 193; 75,76 bin = 0
ACTG1: 75: 1260-1271; 76: 1272-1282 (Actual relative blank position in CDS: 831-858)
ACTB: 75: 1025-1036 76: 1037-1047

NPM1:
CDS length = 882 (885 in fasta), start 302
1-3,24-26,35-37,64-74,83-84,95-100 bin = 0

64-74:858-954(557-653); 95-100: 1132-1183(831-882)

PPIA:
CDS length = 495 (498 in fasta), start 46
30-38,58-71 bin = 0

30-38: 190-233(145-188); 58-71: 332-396(287-351)

# PPT related

```{r}
# Load necessary library
library(ggplot2)

# Original data
original_data <- c(5, 10, 8, 13, 4, 17, 35, 0, 22, 1, 15, 14, 9, 6)

# Grouped sums (intermediate step)
grouped_sums <- c(23, 69, 23, 44)

# Proportions (final transformed data)
transformed_data <- grouped_sums / sum(grouped_sums)

# Create data frames for plotting
original_df <- data.frame(Index = 1:length(original_data), Value = original_data, Category = "Original Data")
transformed_df <- data.frame(Index = 1:length(transformed_data), Value = transformed_data, Category = "Transformed Data")

# Combine data frames for plotting
plot_data <- rbind(
  original_df,
  transformed_df
)

# Plot the distributions with line charts
plot_data %>%
  filter(Category == "Transformed Data") %>%
  ggplot(aes(x = Index, y = Value)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  theme_classic()


```

