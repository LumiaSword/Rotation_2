---
title: "R Notebook"
output: html_notebook
---


# Notes

The second rotation is centered around the downstream data analysis of ribo-seq. Ribosomal analysis was performed by sequencing the Ribosome Protected Footprints (RPFs), which reflects the status of the ribosome through the mRNA fragments undergoing translation, thus obtaining a distribution of reads of a specific length at the transcript level. Data were analyzed using the RiboR software package.

## Updates
1. Review ribosome-profiling. Learn how to use RiboR. Go through tutorial.

2. Use real .ribo data. Filter for the top 100 transcripts with the highest read counts. Check their coverage in each CDS regions. Visualize the distribution.

3. Adjust the way bins are divided.

    before: Position/transcript cds region length to get a relative length. Relative length * 100 and downward rounding to get position assigned to each bin. 
    
    after: A determined number of bins are set. Transcripts with different region lengths have different position ranges in the each bin.
4. For each bins, the distribution of all transcripts is represented by adding up ~~raw counts~~ their respective densities;

    The distribution map is extended to the full transcript range.

    Decide how to set different numbers of bins for different regions.

    Rearrange the order of heatmaprows. Cluster between transcripts.

    Observe outliers transcripts. Try to explain.

5. Backed up using git; add comments; for WDR74 and MTRNR2L, documented and excluded from subsequent analyses

    The correctness of the method was verified using a test set.

    Try to demonstrate the read distribution for higher precision regions.

    If the method is correct, try to explain the high enrichment of readings in the UTR region.

    After confirming the flow of the method, apply it to all 6 experiments.

    Observe if there is a consistent trend in the expression of transcripts across the six experiments?

    How can we programmatically identify differences in trends?
    
    For NPMI-201 and PPIA-204, why they are partially not expressed in the CDS region?

# Packages

```{r}
library(ribor)
library(tidyverse)
library(ggplot2)
library(viridis)
library(pheatmap)
library(grid)
library(ggplotify)
library(cowplot)
library(broom)
library(purrr)
```

# Data sources

Open source data from [NCBI](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158374). .ribo file are generated using [riboflow](https://github.com/ribosomeprofiling/riboflow).

```{r}
h1299.ribo = Ribo("GSE158374_H1299.ribo.hdf5", rename = rename_default )
hek293.ribo = Ribo("GSE158374_manuscript.HEK293.ribo.hdf5",rename = rename_default )
```

# H1299

## Basic info of the .ribo file
```{r}
h1299.ribo

# coverage TRUE, rna TRUE, metadata FALSE
```


```{r,eval = FALSE}
h1299.start = get_metagene(ribo.object = h1299.ribo, 
                           site        = "start",
                           range.lower = 28,
                           range.upper = 32,
                           length      = FALSE,
                           transcript  = FALSE,
                           alias       = TRUE) # Coverage check.

h1299.start[1:5,c(1,2,3,53,54,55)]

head(get_reference_names(h1299.ribo), 2)
```


```{r}
plot_region_counts(x           = h1299.ribo,
                   range.lower = 28,
                   range.upper = 32) # Read Counts for different experiments
```



```{r}
h1299.rc = get_region_counts(h1299.ribo,
                        range.lower = 28, # usually the RPFs has a 28-30nt in length. Use 28-32 for here
                        range.upper = 32,
                        length      = T, # don't separate based on different length
                        transcript  = F, # transcriptome level result
                        alias = T,  # Name of the transcript
                        compact = F) # data.fraame rather than DataFrame
```


```{r, eval=FALSE}
top100_h1299_cds = h1299.rc %>%
  filter(region == "CDS") %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count))%>%
  head(100) %>%
  pull(transcript) # This will return the top 100 transcripts with the most reads in CDS region
```


```{r}
top100_h1299 = h1299.rc %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  head(100) %>%
  pull(transcript) # This will return the top 100 transcripts wih the most reads amoung all region, including UTR5, CDS and UTR3, across all 6 experiments.
```

```{r}
# Check the distribution of those 28-32nt reads in the top 100 broadly
rc_top100 = get_region_counts(h1299.ribo, # return the reads count for UTR, UTRJ, and CDS
                        range.lower = 28,
                        range.upper = 32,
                        length      = T,
                        transcript  = F,
                        alias = T,
                        compact = F,
                        tidy = F) %>%
  filter(transcript %in% top100_h1299)

rc_top100 # for all 6 experiments
```


```{r}
region_coord = get_original_region_coordinates(ribo.object = h1299.ribo , alias = TRUE) # Find the region coordinates for h1299.

top100_region_coord = region_coord %>% # filter for the top 100 transcripts
    filter(transcript %in% top100_h1299) 

# top100_cds_region_coord = region_coord %>%
#     filter(transcript %in% top100_h1299_cds) 

top100_region_coord
```

## Check if the variable indicate the right bonds
```{r,eval=FALSE}
top100_region_coord %>% # manually calculate the original region bonds
  group_by(transcript) %>%
  summarise(UTR5 = UTR5_stop - UTR5_start+1,CDS = CDS_stop - CDS_start+1, UTR3 = UTR3_stop - UTR3_start+1) %>%
  ungroup()
```


```{r,eval=FALSE}
get_original_region_lengths(ribo.object = h1299.ribo, # check with integreted algorithm
                            alias = T) %>%
  separate(transcript, into = paste0("part", 1:5), sep = "\\|", fill = "right") %>%
  filter(part5 %in% top100_h1299) %>%
  select(part5,UTR5,CDS,UTR3) %>%
  arrange(part5)
```

It looks like the bonds is correct.

# A function for top100 distribution in all 6 experiments

```{r}
experiment.info = get_info(ribo.object = h1299.ribo)[['experiment.info']]
has.coverage = experiment.info[experiment.info$coverage == TRUE, "experiment"]
```


To write a function, the intake will be:

    0. An empty list

    1. A list of transcripts name (top 100).

    2. A coordinate df showing transcripts name, CDS start, CDS end, and UTR3 end.

    3. Bin size for each region.

    4. Set of experiments.

The info above, together with the ribo file, can generate coverage df: 

    Coverage df: experiment name, position (from 1 to UTR3 end), count (for reads at each position).



```{r}
# Function to calculate distribution
cov_cal = function(ribo_name,
                   transcripts_names,
                   transcripts_coordinates_df,
                   exp.names,
                   n_bins_utr5,
                   n_bins_cds,
                   n_bins_utr3){
  
  cov_list_exp = list()
  
  for (i in 1:length(exp.names)){
    cov_list = list()
    
    for (j in 1:length(transcripts_names)){
      # cds region bond
      cds_start = transcripts_coordinates_df %>%
        filter(transcript == transcripts_names[j]) %>%
        pull(CDS_start)
      
      cds_end = transcripts_coordinates_df %>%
        filter(transcript == transcripts_names[j]) %>%
        pull(CDS_stop)
      
      # utr5 region bond
      utr5_start = 1
      utr5_end = cds_start - 1
  
      # utr3 region bond
      utr3_start = cds_end + 1
      utr3_end = transcripts_coordinates_df %>%
        filter(transcript == transcripts_names[j]) %>%
        pull(UTR3_stop)
  
      # region length
      utr5_length = utr5_end - utr5_start + 1
      cds_length = cds_end - cds_start + 1
      utr3_length = utr3_end - utr3_start + 1
      
      # bin_size for each transcript
      bin_size_utr5 = utr5_length / n_bins_utr5
      bin_size_cds = cds_length / n_bins_cds
      bin_size_utr3 = utr3_length / n_bins_utr3
      
      # generate the coverage df
      cov_df = get_coverage(ribo.object = ribo_name,
                            name        = transcripts_names[j],
                            range.lower = 28,
                            range.upper = 32,
                            length      = T,
                            alias       = TRUE,
                            tidy        = TRUE,
                            compact     = F,
                            experiment  = exp.names[i])
      
      # reconstruct the coverage df
      cov_df = cov_df %>%
        mutate(position = as.numeric(position)) %>% # as.numeric the position
        mutate(region = case_when(
          position >= utr5_start & position <= utr5_end ~ "UTR5", 
          position >= cds_start & position <= cds_end ~ "CDS",
          position >= utr3_start & position <= utr3_end ~ "UTR3", # assign region for each position
          TRUE ~ NA_character_
          )) %>%
        filter(!is.na(region)) %>%
        mutate(relative_position = case_when(
          region == "UTR5" ~ (position - utr5_start + 1),
          region == "CDS" ~ (position - cds_start + 1),
          region == "UTR3" ~ (position - utr3_start + 1) # calculate the relative position for each region
          )) %>%
        mutate(bin = case_when(
          region == "UTR5" ~ ceiling(relative_position / bin_size_utr5),
          region == "CDS" ~ ceiling(relative_position / bin_size_cds),
          region == "UTR3" ~ ceiling(relative_position / bin_size_utr3) # assign the bin for each position in each region
          ))
      
      # adjust the bin if there are position assigned to bins out of the range
      cov_df$bin[cov_df$region == "UTR5" & cov_df$bin > n_bins_utr5] = n_bins_utr5
      cov_df$bin[cov_df$region == "CDS" & cov_df$bin > n_bins_cds] = n_bins_cds
      cov_df$bin[cov_df$region == "UTR3" & cov_df$bin > n_bins_utr3] = n_bins_utr3
      
      # now the cov_df should have there col: experiment, position, reads, region, relative_position, bin
      # only keep the region and bin, and add transcript for the df
      all_cov = cov_df %>%
        group_by(region, bin) %>%
        summarise(total_count = sum(count),.groups = "drop") %>%
        mutate(transcript = transcripts_names[j]) 
      
      
      # fill the df into the list
      cov_list[[transcripts_names[j]]] = all_cov
    }
    
    # get the df for a certain experiment of all selected transcripts
    coverage_all = bind_rows(cov_list)
    
    coverage_all = coverage_all %>%
      mutate(region = factor(region, levels = c("UTR5", "CDS", "UTR3"))) # the following analysis can thus have the order 
    
    # save the coverage data for the current experiment
    cov_list_exp[[exp.names[i]]] = coverage_all
  }
  
  # return the final list containing coverage data for all experiments
  return(cov_list_exp)
}
```


```{r}
h1299_cov = cov_cal(ribo_name = h1299.ribo,
            transcripts_names = top100_h1299,
            transcripts_coordinates_df = top100_region_coord,
            exp.names = has.coverage,
            n_bins_utr5 = 30,
            n_bins_cds = 100,
            n_bins_utr3 = 30)
```

## To manually check whether the function works

```{r,eval=FALSE}
cov_list = list()

n_bins_utr5 = 30
n_bins_cds = 100
n_bins_utr3 = 30 # different bins number set for different region

# experiment: 20210318-NSP1-H1299-A
for (i in 1:length(top100_h1299)) {
  # cds region bond
  cds_start = top100_region_coord %>%
    filter(transcript == top100_h1299[i]) %>%
    pull(CDS_start)
  
  cds_end = top100_region_coord %>%
    filter(transcript == top100_h1299[i]) %>%
    pull(CDS_stop)
  
  # utr5 region bond
  utr5_start = 1
  utr5_end = cds_start - 1
  
  # utr3 region bond
  utr3_start = cds_end + 1
  utr3_end = top100_region_coord %>%
    filter(transcript == top100_h1299[i]) %>%
    pull(UTR3_stop)
  
  utr5_length = utr5_end - utr5_start + 1
  cds_length = cds_end - cds_start + 1
  utr3_length = utr3_end - utr3_start + 1
  
  bin_size_utr5 = utr5_length / n_bins_utr5
  bin_size_cds = cds_length / n_bins_cds
  bin_size_utr3 = utr3_length / n_bins_utr3
  
  cov_df = get_coverage(ribo.object = h1299.ribo,
                    name        = top100_h1299[i],
                    range.lower = 28,
                    range.upper = 32,
                    length      = T,
                    alias       = TRUE,
                    tidy        = TRUE,
                    compact     = F,
                    experiment  = has.coverage[6])
  
  cov_df = cov_df %>%
    mutate(position = as.numeric(position)) %>%
    mutate(region = case_when(
      position >= utr5_start & position <= utr5_end ~ "UTR5",
      position >= cds_start & position <= cds_end ~ "CDS",
      position >= utr3_start & position <= utr3_end ~ "UTR3",
      TRUE ~ NA_character_
    )) %>%
    filter(!is.na(region)) %>%
    mutate(relative_position = case_when(
      region == "UTR5" ~ (position - utr5_start + 1),
      region == "CDS" ~ (position - cds_start + 1),
      region == "UTR3" ~ (position - utr3_start + 1)
    )) %>%
    mutate(bin = case_when(
      region == "UTR5" ~ ceiling(relative_position / bin_size_utr5),
      region == "CDS" ~ ceiling(relative_position / bin_size_cds),
      region == "UTR3" ~ ceiling(relative_position / bin_size_utr3)
    ))
  
  cov_df$bin[cov_df$region == "UTR5" & cov_df$bin > n_bins_utr5] = n_bins_utr5
  cov_df$bin[cov_df$region == "CDS" & cov_df$bin > n_bins_cds] = n_bins_cds
  cov_df$bin[cov_df$region == "UTR3" & cov_df$bin > n_bins_utr3] = n_bins_utr3
  
  all_cov = cov_df %>%
    group_by(region, bin) %>%
    summarise(total_count = sum(count),.groups = "drop") %>%
    mutate(transcript = top100_h1299[i]) 
    
  
  cov_list[[top100_h1299[i]]] = all_cov
  
}


coverage_all = bind_rows(cov_list)

coverage_all = coverage_all %>%
  mutate(region = factor(region, levels = c("UTR5", "CDS", "UTR3")))
```


```{r,eval=FALSE}
all(coverage_all == h1299_cov[[6]])
```

# Visualize

## Distribution, by density
```{r}
# The first experiment, as an example, 20210318-NSP1-H1299-A
h1299_cov_exp1 = h1299_cov[[1]]

h1299_cov_exp1 %>%
  group_by(transcript) %>%
  mutate(density = total_count / sum(total_count))  %>% # density will turn NA if total_count in certain region to be 0
  ungroup() %>%
  group_by(region,bin) %>%
  summarise(count = sum(density,na.rm = T),.groups = "drop") %>%
  ggplot(aes(x = bin, y = count)) +
  geom_point() + 
  geom_line() +
  labs(
    x = "Bin",
    y = "Sum Read Count Density"
  ) +
  facet_grid(~ region, scales = "free_x", space = "free")

# coverage_all %>%
#   group_by(region,bin) %>%
#   summarise(count = sum(total_count),.groups = "drop") %>%
#   ggplot(aes(x = bin, y = count)) +
#   geom_line() +
#   labs(
#     x = "Bin",
#     y = "Sum Read Count"
#   ) +
#   facet_wrap(~ region, scales = "free_x")
```

> Would it be a good idea to use density to represent the reads number in each bin of each transcript in a heatmap? No.

```{r,fig.width= 10,fig.height= 15}
# combined version for all six experiments
combined_cov = bind_rows(
  lapply(1:length(h1299_cov), function(i) {
    h1299_cov[[i]] %>%
      mutate(experiment = has.coverage[i])
  })
)

combined_cov %>%
  group_by(transcript, experiment) %>%
  mutate(density = total_count / sum(total_count)) %>%
  ungroup() %>%
  group_by(region, bin, experiment) %>%
  summarise(count = sum(density, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = bin, y = count)) +
  geom_line() +
  labs(
    x = "Bin",
    y = "Sum Read Count Density"
  ) +
  facet_grid(experiment ~ region, scales = "free_x", space = "free") # each row an experiment

```




## Heatmap Distribution

```{r}
h1299_cov_exp1 %>%
  mutate(transcript = factor(transcript, levels = rev(top100_h1299)))%>%
  ggplot(aes(x = bin, y = transcript, fill = log10(total_count+1)))+
  geom_tile() +
  labs(
    x = "Bin",
    y = "Transcript",
    fill = "Read Count")+
  scale_fill_viridis_c()  + 
  facet_grid(~ region, scales = "free_x", space = "free")
```


```{r}
# Heatmap clustering on trancripts
# write a function

heatmap_cluster = function(exp_data, exp_name, sorted_transcripts_names) {
  exp_data_wide = exp_data %>%
    group_by(transcript) %>%
    arrange(region) %>%
    mutate(transcript = factor(transcript, levels = sorted_transcripts_names)) %>%
    arrange(transcript) %>%
    pivot_wider(names_from = c(region, bin), values_from = total_count) # into wider form of df
  
  exp_data_matrix = as.matrix(exp_data_wide[,-1])
  rownames(exp_data_matrix) <- exp_data_wide$transcript # rm transcript col
  
  annotation_col = data.frame(
    Region = ifelse(grepl("UTR5", colnames(exp_data_matrix)), "UTR5",
                    ifelse(grepl("CDS", colnames(exp_data_matrix)), "CDS", "UTR3")) # annotate the region for the final pheatmap
  )
  rownames(annotation_col) <- colnames(exp_data_matrix)
  
  # generate heatmap
  heatmap_plot = pheatmap(
    log10(exp_data_matrix + 1),
    cluster_rows = TRUE,
    cluster_cols = F, # cluster based on rows
    show_rownames = TRUE,
    show_colnames = F, # show the transcripts' name
    color = viridis::viridis(50),
    annotation_col = annotation_col,
    main = paste(exp_name, ": Transcripts Read Count Heatmap"),
    silent = TRUE # return no plot 
  )
  
  heatmap_plot = as.ggplot(heatmap_plot)
  
  return(heatmap_plot)
}
```


```{r}
h1299_exp1_heatmap = heatmap_cluster(h1299_cov_exp1,has.coverage[1],top100_h1299)

h1299_exp1_heatmap
```


```{r,fig.height=20}
heatmap_list = lapply(1:6, function(i) heatmap_cluster(h1299_cov[[i]], has.coverage[i],top100_h1299))
plot_grid(plotlist = heatmap_list, ncol = 3, align = 'v')
```


# Troubleshooting typical anomalies

We can easily observe that MTRNR2L transcripts and WDR74 lack of reading in all 6 experiments. The reason is :


We should exclude these transcripts and go through the whole process again.

```{r}
# Re-select the transcripts

excluded_transcripts = h1299.rc %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  head(104) %>%
  filter(str_detect(transcript,c("MTRNR")) | str_detect(transcript,"WDR")) %>%
  pull(transcript)

top100_h1299_mod = h1299.rc %>%
  group_by(transcript) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  head(104) %>%
  pull(transcript) # pull the top 104

top100_h1299_mod = setdiff(top100_h1299_mod,excluded_transcripts) # filter out the excluded outliers

top100_region_coord_mod = region_coord %>%
  filter(transcript %in% top100_h1299_mod)
```


```{r}
h1299_cov_mod = cov_cal(ribo_name = h1299.ribo,
            transcripts_names = top100_h1299_mod,
            transcripts_coordinates_df = top100_region_coord_mod,
            exp.names = has.coverage,
            n_bins_utr5 = 30,
            n_bins_cds = 100,
            n_bins_utr3 = 30)
```

```{r,fig.height=30}
# Regenerate the Heatmap with the new objects
mod_heatmap_list = lapply(1:6, function(i) heatmap_cluster(h1299_cov_mod[[i]], has.coverage[i],top100_h1299_mod))
plot_grid(plotlist = mod_heatmap_list, ncol = 3, align = 'v')
```

# Are the transcripts distributed similarly across all the experiments.

How we define whether the reads distribution of the transcripts remain similar or varies? We can first combine the df

```{r}
combined_cov_h1299_mod = bind_rows(
  lapply(1:length(h1299_cov_mod), function(i) {
    h1299_cov_mod[[i]] %>%
      mutate(experiment = has.coverage[i])
  })
)
```


We can observe that the transcripts with the highest number of readings in the CDS area are, for example

EEF1A1-202,HSP90AA1-201,GAPDH-201,VIM-209,HSP90AB1-202,HSPA8-224

Among them, EEF1A1-202 have the most reads, and always have a separate branch across all 6 transcripts (6/6), and remain the brightest color; HSP90AA1-201 are always clustered together with VIM-209 (3/6) or GAPDH-201 (2/6) -- at least they are always arranged close to each other (5/6); HSP90AB1-202 and HSPA8-224 are always clustered together (6/6).

We then check the ones that has significant deletion in CDS: NPM1-201 and PPIA-204; and the one have part of deletion in CDS: ACTB-201, ACTG1-210, PABPC1-201

In all exps, NPM1-201 and PPIA-204 are clustered together, and their deletion in reads distribution remain consistent; ACTB-201 and ACTG1-210 are usually (4/6) clustered together, as they both have no reads around bin 75. PABPC1-201 have the "deletion" in a some how different area in CDS region, but still consistent across 6 exps.

There are some interesting ones: 

HSPA5-201 and ACTG1-201 have some accumulation of reads in the 5' end; 

ACTB-201 and ACTG1-201 have this accumulation around start -- which is true, but their accumulation seems higher in UTR5 region, rather than CDS region, by raw counts

HMGB1-203 have an accumulation in the 3' end, in the very last bin.

Here's the heatmap that can show the designated distribution of a single transcript across 6 experiments

```{r,fig.width=15,fig.height= 5}
combined_cov_h1299_mod %>%
  mutate(transcript = factor(transcript, levels = rev(top100_h1299))) %>%
  filter(str_detect(transcript,"NPM1|PPIA|PABPC1|HMGB1|ACTB")) %>%
  ggplot(aes(x = bin, y = experiment, fill = log10(total_count+1)))+
  geom_tile() +
  labs(
    x = "Bin",
    y = "Experiment",
    fill = "Read Count")+
  scale_fill_viridis_c()  + 
  facet_grid(transcript ~ region, scales = "free_x", space = "free") # so result can be separated by transcripts, by rows.
```

This is intuitive, but also subjective. How we can do this in an objective way


```{r}
combined_cov_h1299_mod %>%
  spread(key = experiment, value = total_count) %>%
  group_by(transcript, region) %>%
  summarise(
    correlation_value = {
      # manually choose the numeric cols
      experiment_data = select(cur_data(), starts_with("2021"))
      
      # correlate
      correlation_matrix = cor(as.matrix(experiment_data), use = "pairwise.complete.obs", method = "spearman") # We use Spearman correlation here. 
      
      # `use` is how we treat the NAs. Here will compute all possible non-NA pairs.
      
      # As we want to focus on whether the trend of the transcript varies across all experiments, and would not like to be affected by the difference in scale, we use Spearman.
      
      # Take the upper part of the correlation matrix and calculate the mean.
      mean(correlation_matrix[upper.tri(correlation_matrix)], na.rm = TRUE) # Generate one correlation value for each region. 3 for a transcript.
    }
  ) %>%
  ungroup() %>%
  arrange(correlation_value) %>% # We see a lot of UTR region having high variability. We want to focus on the CDS region.
  filter(region == "CDS")
```

As for the CDS region, evne the most variable one got > 0.8 in correlation_value, which means they are quite consistent across all 6 experiments.


